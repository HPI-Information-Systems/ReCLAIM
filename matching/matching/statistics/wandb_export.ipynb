{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Runs without pretraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/\".join(os.getcwd().split(\"/\")[:-2]))\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "from matching.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Please set the slurmIds of the runs you want to compare here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_ids = [1064635, 1064640, 1064113]\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading run data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(precision, recall):\n",
    "    return (2 * (precision * recall) / (precision + recall)).round(3)\n",
    "\n",
    "\n",
    "colour_train = \"#F79647\"\n",
    "colour_pretrain = \"#5D97BF\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 20})\n",
    "plt.rcParams.update({\"axes.titlesize\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models not using pretraining\n",
    "\n",
    "Loads all data of runs that do not use pretraining and calculates their average performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = api.runs(f\"{settings.wandb_entity}/{settings.wandb_project}\")\n",
    "\n",
    "name_list = []\n",
    "precision, recall, f1, best_threshold = [], [], [], []\n",
    "seeds = []\n",
    "dropout = []\n",
    "epochs = []\n",
    "optimizer = []\n",
    "\n",
    "for run in runs:\n",
    "    if not run.state == \"finished\":\n",
    "        continue\n",
    "    summary = run.summary._json_dict\n",
    "    if \"test_precision\" not in summary:\n",
    "        continue\n",
    "    if \"test_recall\" not in summary:\n",
    "        continue\n",
    "    if \"test_f1\" not in summary:\n",
    "        continue\n",
    "    if \"best_threshold\" not in summary:\n",
    "        summary[\"best_threshold\"] = 0.5\n",
    "\n",
    "    name_list.append(run.name)\n",
    "\n",
    "    precision.append(summary[\"test_precision\"])\n",
    "    recall.append(summary[\"test_recall\"])\n",
    "    f1.append(summary[\"test_f1\"])\n",
    "    best_threshold.append(summary[\"best_threshold\"])\n",
    "    seeds.append(run.name.split(\"seed=\")[1].split(\"_\")[0])\n",
    "    if \"drop\" in run.config:\n",
    "        dropout.append(run.config[\"drop\"])\n",
    "    else:\n",
    "        dropout.append(0.0)\n",
    "\n",
    "    if \"epochs\" in run.config:\n",
    "        epochs.append(run.config[\"epochs\"])\n",
    "    else:\n",
    "        if \"n_epochs\" in run.config:\n",
    "            epochs.append(run.config[\"n_epochs\"])\n",
    "        else:\n",
    "            epochs.append(10)\n",
    "\n",
    "    if \"adamw\" in run.config and run.config[\"adamw\"] == True:\n",
    "        optimizer.append(\"adamw\")\n",
    "    else:\n",
    "        optimizer.append(\"adam\")\n",
    "\n",
    "\n",
    "runs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": name_list,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"seed\": seeds,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"dropout\": dropout,\n",
    "    }\n",
    ")\n",
    "\n",
    "lm_type = [\n",
    "    \"roberta\",\n",
    "    \"distilbert\",\n",
    "]\n",
    "\n",
    "runs_df[\"lm\"] = runs_df[\"name\"].apply(\n",
    "    lambda x: next((lm for lm in lm_type if lm in x), \"unknown\")\n",
    ")\n",
    "runs_df[\"name\"] = runs_df.apply(lambda row: f\"{row['name']}\", axis=1)\n",
    "\n",
    "runs_df[\"run_id\"] = runs_df[\"name\"].apply(lambda x: x.split(\"slurmId=\")[1])\n",
    "\n",
    "\n",
    "runs_df[\"model\"] = runs_df[\"name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "runs_df.sort_values(\"run_id\", ascending=False, inplace=True)\n",
    "\n",
    "runs_df[\"run_id\"] = runs_df[\"run_id\"].astype(int)\n",
    "\n",
    "runs_df = runs_df[runs_df[\"run_id\"].isin(compare_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision = runs_df.groupby(\"run_id\")[\"precision\"].mean().round(3)\n",
    "avg_recall = runs_df.groupby(\"run_id\")[\"recall\"].mean().round(3)\n",
    "\n",
    "pd.concat([avg_precision, avg_recall], axis=1)\n",
    "\n",
    "f1 = pd.DataFrame(\n",
    "    index=runs_df[\"run_id\"].unique(),\n",
    ")\n",
    "\n",
    "f1[\"f1\"] = (2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)).round(3)\n",
    "\n",
    "f1[\"avg_precision\"] = avg_precision\n",
    "f1[\"avg_recall\"] = avg_recall\n",
    "\n",
    "\n",
    "f1[\"model\"] = runs_df.groupby(\"run_id\")[\"model\"].first()\n",
    "f1[\"lm\"] = runs_df.groupby(\"run_id\")[\"lm\"].first()\n",
    "\n",
    "f1[\"run_id\"] = f1.index\n",
    "f1[\"epochs\"] = runs_df.groupby(\"run_id\")[\"epochs\"].first()\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models using pretraining\n",
    "\n",
    "Loads all data of runs that use pretraining and calculates their average performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = api.runs(f\"{settings.wandb_entity}/{settings.wandb_pretrain_project}\")\n",
    "\n",
    "name_list = []\n",
    "precision, recall, f1, best_threshold = [], [], [], []\n",
    "batch_sizes, comment = [], []\n",
    "linear = []\n",
    "epochs = []\n",
    "seeds = []\n",
    "pretrain_epochs = []\n",
    "dropout = []\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    if not run.state == \"finished\":\n",
    "        continue\n",
    "    summary = run.summary._json_dict\n",
    "    if \"test_precision\" not in summary:\n",
    "        continue\n",
    "    if \"test_recall\" not in summary:\n",
    "        continue\n",
    "    if \"test_f1\" not in summary:\n",
    "        continue\n",
    "    if \"best_threshold\" not in summary:\n",
    "        summary[\"best_threshold\"] = 0.5\n",
    "\n",
    "    if not \"slurmId=\" in run.name:\n",
    "        continue\n",
    "\n",
    "    name_list.append(run.name)\n",
    "\n",
    "    precision.append(summary[\"test_precision\"])\n",
    "    recall.append(summary[\"test_recall\"])\n",
    "    f1.append(summary[\"test_f1\"])\n",
    "    best_threshold.append(summary[\"best_threshold\"])\n",
    "\n",
    "    config = run.config\n",
    "    if \"batch_size\" in config:\n",
    "        batch_sizes.append(config[\"batch_size\"])\n",
    "    else:\n",
    "        batch_sizes.append(32)\n",
    "    if \"comment\" in config:\n",
    "        comment.append(config[\"comment\"])\n",
    "    else:\n",
    "        comment.append(None)\n",
    "\n",
    "    if \"linear\" in config:\n",
    "        linear.append(config[\"linear\"])\n",
    "    else:\n",
    "        linear.append(False)\n",
    "\n",
    "    if \"epochs\" in config:\n",
    "        epochs.append(config[\"epochs\"])\n",
    "    else:\n",
    "        epochs.append(10)\n",
    "\n",
    "    if \"pretrain_epochs\" in config:\n",
    "        pretrain_epochs.append(config[\"pretrain_epochs\"])\n",
    "    else:\n",
    "        pretrain_epochs.append(10)\n",
    "\n",
    "    if \"dropout\" in config:\n",
    "        dropout.append(config[\"dropout\"])\n",
    "    else:\n",
    "        dropout.append(0.0)\n",
    "\n",
    "    seeds.append(run.name.split(\"seed=\")[1].split(\"_\")[0])\n",
    "\n",
    "\n",
    "pretrain_runs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": name_list,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"batch_size\": batch_sizes,\n",
    "        \"comment\": comment,\n",
    "        \"linear\": linear,\n",
    "        \"seed\": seeds,\n",
    "        \"epochs\": epochs,\n",
    "        \"pretrain_epochs\": pretrain_epochs,\n",
    "        \"dropout\": dropout,\n",
    "    }\n",
    ")\n",
    "\n",
    "lm_type = [\n",
    "    \"roberta\",\n",
    "    \"distilbert\",\n",
    "]\n",
    "\n",
    "pretrain_runs_df[\"lm\"] = pretrain_runs_df[\"name\"].apply(\n",
    "    lambda x: next((lm for lm in lm_type if lm in x), \"unknown\")\n",
    ")\n",
    "\n",
    "\n",
    "pretrain_runs_df[\"run_id\"] = pretrain_runs_df[\"name\"].apply(\n",
    "    lambda x: x.split(\"slurmId=\")[1]\n",
    ")\n",
    "\n",
    "\n",
    "pretrain_runs_df[\"model\"] = pretrain_runs_df[\"name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "pretrain_runs_df[\"run_id\"] = pretrain_runs_df[\"run_id\"].astype(int)\n",
    "\n",
    "pretrain_runs_df.sort_values(\"run_id\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "pretrain_runs_df[\"run_id\"] = pretrain_runs_df[\"run_id\"].astype(int)\n",
    "pretrain_runs_df = pretrain_runs_df[pretrain_runs_df[\"run_id\"].isin(compare_ids)]\n",
    "pretrain_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision = pretrain_runs_df.groupby(\"run_id\")[\"precision\"].mean().round(3)\n",
    "avg_recall = pretrain_runs_df.groupby(\"run_id\")[\"recall\"].mean().round(3)\n",
    "\n",
    "pretrain_f1 = pd.DataFrame(\n",
    "    index=pretrain_runs_df[\"run_id\"].unique(),\n",
    ")\n",
    "\n",
    "pretrain_f1[\"f1\"] = calc_f1(avg_precision, avg_recall)\n",
    "\n",
    "pretrain_f1[\"avg_precision\"] = avg_precision\n",
    "pretrain_f1[\"avg_recall\"] = avg_recall\n",
    "\n",
    "\n",
    "pretrain_f1[\"model\"] = pretrain_runs_df.groupby(\"run_id\")[\"model\"].first()\n",
    "pretrain_f1[\"lm\"] = pretrain_runs_df.groupby(\"run_id\")[\"lm\"].first()\n",
    "pretrain_f1[\"run_id\"] = pretrain_f1.index\n",
    "\n",
    "pretrain_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = runs_df.merge(\n",
    "    pretrain_runs_df, on=[\"run_id\", \"seed\", \"epochs\"], suffixes=(\"\", \"_pretrain\")\n",
    ")\n",
    "\n",
    "relevant = joined[joined[\"run_id\"].isin(compare_ids)]\n",
    "\n",
    "\n",
    "# Reorder the columns in the relevant DataFrame\n",
    "column_order = [\n",
    "    \"lm\",\n",
    "    \"f1\",\n",
    "    \"f1_pretrain\",\n",
    "    \"precision\",\n",
    "    \"precision_pretrain\",\n",
    "    \"recall\",\n",
    "    \"recall_pretrain\",\n",
    "    \"best_threshold\",\n",
    "    \"best_threshold_pretrain\",\n",
    "    \"comment\",\n",
    "    \"linear\",\n",
    "    \"pretrain_epochs\",\n",
    "    \"lm_pretrain\",\n",
    "    \"run_id\",\n",
    "    \"seed\",\n",
    "    \"batch_size\",\n",
    "    \"dropout\",\n",
    "    \"epochs\",\n",
    "]\n",
    "relevant = relevant[column_order]\n",
    "\n",
    "\n",
    "relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = relevant.groupby(\"run_id\")\n",
    "\n",
    "grouped_data = grouped.agg(\n",
    "    {\n",
    "        \"precision\": \"mean\",\n",
    "        \"recall\": \"mean\",\n",
    "        \"precision_pretrain\": \"mean\",\n",
    "        \"recall_pretrain\": \"mean\",\n",
    "        \"f1\": \"mean\",\n",
    "        \"best_threshold\": \"mean\",\n",
    "        \"batch_size\": \"first\",\n",
    "        \"comment\": \"first\",\n",
    "        \"linear\": \"first\",\n",
    "        \"pretrain_epochs\": \"first\",\n",
    "        \"epochs\": \"first\",\n",
    "        \"lm\": \"first\",\n",
    "        \"dropout\": \"first\",\n",
    "    }\n",
    ")\n",
    "\n",
    "grouped_data = grouped_data.reset_index()\n",
    "\n",
    "\n",
    "avg_precision = grouped_data[\"precision\"]\n",
    "avg_recall = grouped_data[\"recall\"]\n",
    "\n",
    "avg_precision_pretrain = grouped_data[\"precision_pretrain\"]\n",
    "avg_recall_pretrain = grouped_data[\"recall_pretrain\"]\n",
    "\n",
    "f1 = calc_f1(avg_precision, avg_recall)\n",
    "f1_pretrain = calc_f1(avg_precision_pretrain, avg_recall_pretrain)\n",
    "\n",
    "joint_f1 = pd.concat(\n",
    "    [avg_precision, avg_recall, avg_precision_pretrain, avg_recall_pretrain], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "joint_f1[\"f1\"] = f1\n",
    "joint_f1[\"f1_pretrain\"] = f1_pretrain\n",
    "joint_f1[\"percent_improvement\"] = ((f1_pretrain - f1) / f1) * 100\n",
    "joint_f1[\"absolute_improvement\"] = f1_pretrain - f1\n",
    "\n",
    "joint_f1[\"dropout\"] = grouped_data[\"dropout\"]\n",
    "joint_f1[\"epochs\"] = grouped_data[\"epochs\"]\n",
    "joint_f1[\"pretrain_epochs\"] = grouped_data[\"pretrain_epochs\"]\n",
    "joint_f1[\"batch_size\"] = grouped_data[\"batch_size\"]\n",
    "joint_f1[\"lm\"] = grouped_data[\"lm\"]\n",
    "\n",
    "joint_f1[\"run_id\"] = grouped_data[\"run_id\"]\n",
    "joint_f1.set_index(\"run_id\", inplace=True)\n",
    "\n",
    "\n",
    "joint_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss curves\n",
    "\n",
    "Extracts data on the loss curves of a particular run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "slurmId = \"1064640\"\n",
    "seed = \"0\"\n",
    "\n",
    "runs = api.runs(\"bp2023fn1-kunstgraph/Matching-Pretrain\")\n",
    "matching_runs = []\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    # if not run.state == \"finished\":\n",
    "    #     continue\n",
    "\n",
    "    if f\"slurmId={slurmId}\" in run.name and f\"seed={seed}\" in run.name:\n",
    "        history = run.history()\n",
    "        if \"train_loss\" in history.columns:\n",
    "            train_loss = history[\"train_loss\"].dropna().tolist()\n",
    "            train_losses.append(\n",
    "                {\n",
    "                    \"run_id\": run.id,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"pretrain_loss\": history[\"pretrain_loss\"].dropna().tolist(),\n",
    "                    \"type\": (\n",
    "                        \"Pretrain\" if \"Matching-Pretrain\" in run.project else \"Regular\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "runs = api.runs(\"bp2023fn1-kunstgraph/Matching\")\n",
    "\n",
    "for run in runs:\n",
    "    if not run.state == \"finished\":\n",
    "        continue\n",
    "    if f\"slurmId={slurmId}\" in run.name and f\"seed={seed}\" in run.name:\n",
    "\n",
    "        history = run.history()\n",
    "        if \"train_loss\" in history.columns:\n",
    "            train_loss = history[\"train_loss\"].dropna().tolist()\n",
    "            train_losses.append(\n",
    "                {\n",
    "                    \"run_id\": run.name,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"type\": (\n",
    "                        \"Pretrain\" if \"Matching-Pretrain\" in run.project else \"Regular\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "losses = pd.DataFrame(train_losses)\n",
    "losses.set_index(\"run_id\", inplace=True)\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Learning loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6.5, 5))\n",
    "\n",
    "for idx, row in losses.iterrows():\n",
    "    if row[\"type\"] == \"Pretrain\":\n",
    "        plt.plot(row[\"train_loss\"], label=f\"P-Ditto-D\")\n",
    "    else:\n",
    "        plt.plot(row[\"train_loss\"], label=f\"Ditto-D\")\n",
    "\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../data/output/plots/loss_curve.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6.5, 5))\n",
    "\n",
    "for idx, row in losses.iterrows():\n",
    "    if row[\"type\"] == \"Pretrain\":\n",
    "        plt.plot(row[\"pretrain_loss\"], label=f\"P-Ditto-D\")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.savefig(\"../../data/output/plots/pretrain_loss_curve.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
